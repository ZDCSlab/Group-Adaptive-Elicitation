model_path: checkpoints/meta-llama/Llama-3.1-8B
model_name: Llama-3.1-8B
epochs: 10000
batch_size: 2
block_size: 1024
lr: 1e-4
grad_accum_steps: 8
betas: [0.9, 0.95]
dtype: bfloat16
weight_decay: 0.01 
seed: 42
eval_interval: 200
grad_clip: 1.0
peft: True 
r: 8
lora_alpha: 24
lora_dropout: 0.1 
target_modules: ["q_proj", "v_proj"]